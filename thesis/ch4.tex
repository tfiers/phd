
\section{Introduction}

This chapter introduces three new methods to detect synaptic connections from voltage signals.

The first two methods are still based on the STA (spike-triggered average), as in the last chapter. And, also as before, they still use shuffled presynaptic spiketrains\footnote{
    A note on terminology: when we write `presynaptic' or `postsynaptic' in this thesis, we often mean the neuron that we are testing as a \emph{possible} presynaptic / postsynaptic connection.
}
to provide a null-distribution for the test statistic

The difference is in how the STA is used. In the previous chapter we used the height of the STA (`peak-to-peak', ptp) as a test statistic.\\
The first new method here instead \emph{correlates} the STA with some template of what the STA of a true connection would look like, to calculate a test statistic.\\
The second method fits an idealized function to the STA, and uses the goodness-of-fit as a test statistic.

The third method steps away entirely from the STA, and instead uses the data of the different spike-triggered windows directly, without averaging them together in an STA.

% Actually, linear regression on sep windows is same as linreg on the average, it seems after quickly writing out the normal equations (solution for βhat) for both.





\section{STA Template correlation}

% \begin{itemize}
%     \item Idea for the two-pass test
%     \item Template examples. Both ideal and template found with first-pass (STA height-only)
%     \item Peformance for different N, \& comparison with previous method
% \end{itemize}

The idea behind this connection test is as follows: the more our actual STA looks like some idealized, `clean' STA, the more likely the connection exists. We quantify this `looking like' here by a simple Pearson correlation.

The question then remains what to correlate our real STA with. An ideal template to correlate with would be the average STA of all true connections in the network.\footnote
{Specifically, either all excitatory connections, or all inhibitory connections.
When correlating some connection's STA with an 'excitatory'  (upwards) template, the sign of the correlation tells us whether the connection is excitatory ($+$) or inhibitory ($–$).}
The knowledge of these connections is of course not available a-priori (it is what we are trying to infer). But it turns out we can obtain a signal that is very close to this ideal template with a two-step approach.

In the first step, we use the previous test statistic (peak-to-peak height of the STA); but with a stricter $p$-value threshold (a higher $α$). This gives us a sample of true connections with few false positives (but with many missed connections). The STAs of these found connections are then averaged, to obtain an estimate for the STA template. We find that this template matches the ideal template closely (see \cref{fig:corr}, right).

In the second step, we correlate this found template with the STAs of all possible connections (and with their shuffle-control counterparts). The correlation values values are then used as test statistic, with the original $α = 0.05$ threshold.

% \marginpar{
%     \hspace{-1.42em}
%     \includegraphics[w=1.15]{corr-overlay}
%     \captionof{figure}{An actual STA (orange), and the template it is correlated with (blue).}
%     \label{fig:corr-overlay}
% }

\begin{figure}
    \subfloat{
        \includegraphics[w=0.7]{corr-overlay.png}
        \includegraphics[w=0.7]{corr2.png}
    }
    \captionn[. ]
    {Correlating the STA with a template}
    {\Left: An example STA, and the template it will be correlated with to calculate the connection test statistic.\\
    \Right: Two possible STA templates: in blue, the average of the STAs of many true excitatory connections (namely of 50 neurons -- part of a 1000-neuron recurrent network -- that all had their voltage recorded; and all their excitatory inputs).
    In orange, the average STA of the excitatory connections detected with a strict `peak-to-peak' test.}
    \label{fig:corr}
\end{figure}

We find that this correlation-based test statistic outperforms the simple peak-to-peak measure (\cref{fig:N_sweep__AUC__template-corr_vs_STA}).

\begin{figure}
    \subfloat{
        \includegraphics[w=0.72]{N_sweep__AUC__STA_height}
        \includegraphics[w=0.72]{N_sweep__AUC__template-corr}
    }
    \captionn
    {Correlation outperforms STA height as connection test}
    {Compare also with \cref{fig:N_sweep__AUC__upstroke_vs_STA}.\\
    AdEx neuron, 10-minute recording, no voltage imaging noise,  5 different seeds (blue dots). All inputs are tested (instead of just the highest firing ones), in addition to 100 random unconnected spiketrains. AUC chance level is at $0.252$ (as in \cref{fig:AUC_chance_level}). Note that for the two highest numbers of inputs $N$, the AUC of the template correlation method falls below chance level. [This is suspicious and must be investigated]. \\
    Source: \nburl{2023-04-11__Nto1_AdEx_conntest_methods_comparison}.}
    \label{fig:N_sweep__AUC__template-corr_vs_STA}
\end{figure}



\FloatBarrier
\section{Fitting a full STA model}

% \begin{itemize}
%     \item Model design
%     \item Iterative model fitting
%     \item Problem of overfitting, and parameter-constraints to solve it
%     \item An advantage: fit parameters (like transmission delay and time constants) are biologically ~meaningful
%     \item Perfomance for different N, \& comparison with previous methods
% \end{itemize}

Instead of a data-driven approach for the `ideal' STA shape, for this method we manually design a function (a 7-parameter function that we'll call $f$), which we will fit to the actual STAs.

\marginpar{
    \vspace*{-5em}
    \hspace*{-1.6em}
    \input{figs/PSP.tex}
    \captionof{figure}{\small A simple PSP model.}
    \label{fig:alpha-function}
    \vspace*{2em}
}

One part of this STA model function is shaped like a postsynaptic potential bump (PSP, \cref{fig:alpha-function}). It is the impulse response of two linear integrators placed in series. Or, in other words, the convolution of two `step-and-decay' functions.\footnote{
    `Step-and-exponential-decay': $u(t) · e^{–t/τ}$, with $u(t)$ the Heaviside step function $\bm{1}_{t ≥ 0}$.
}%
$^{,}$  % comma between footnotemarks
\footnoteatfoot{
        In computational neuroscience, this function is sometimes called the `alpha function' or `alpha synapse'. \emph{Synapse}, because the double integrator model can be used to model synaptic conductance ($g$), with then a fast rise τ₁ (modelling neurotransmitter release), and a slower decay τ₂ (modelling its dissipation). The synaptic conductance model in our simulation is simpler: the `rise' is instant, i.e. we only have exponential decay.
}
It is the postsynaptic potential in the simplest linear neuron model ($\d{v} · τₘ = -v + I$) where the synaptic current $I$ is also linear ($\d{I} · τₛ = -I + s$).\footnote{
    $τₛ$ and $τₘ$ are the synaptic and membrane time constants, and
    $s(t) = \sumᵢ δ(t - tᵢ)$ is the train of input spikes $i$.
}

Solving for $v$ and with a single input spike at $t = 0$, we have, for $t ≥ 0$:
\begin{equation}
    \text{PSP}(t) = \begin{cases}
        t\, e^{-t/τₘ}   &\hspace{8pt} τₘ = τₛ \\
        \frac{τₘτₛ}{τₘ–τₛ} \left( e^{–t/τₘ} – e^{–t/τₛ} \right)  &\hspace{8pt} τₘ ≠ τₛ
    \end{cases}
    \label{eq:PSP}
\end{equation}

We find that this function approximates the shape of the \emph{simulated} postsynaptic potential in our actual neuron model well  -- even though our neuron model is not so linear.\footnote
    {Our model for the membrane potential has a component (the adaptation current $u$) whose differential equation recurrently depends on $v$. Additionally, the synaptic currents $Iⱼ$ are \emph{conductance-based}, meaning that they also recurrently depend on $v$: $Iⱼ = gⱼ \, (v – Eⱼ)$ (with $Eⱼ$ the reversal potential at synapse $j$, and the synaptic conductance $gⱼ$ a linear integrator of the input spike train $sⱼ(t)$:\newline
    $\d{gⱼ} · τ_{s,j} = – gⱼ + sⱼ$).}

But even though \cref{eq:PSP} models our simulated \emph{PSPs} well, it does not resemble our \emph{STAs}\footnote{
    This falsifies our initial implicit hypothesis that STAs are merely noisy reflections of PSPs.
};
see for example the average STAs in \cref{fig:corr}.
First, the bump in the STA does not occur immediately after the presynaptic neuron's spike. This is due to the simulated axonal transmission delay, and it is easily replicated in the model by shifting the PSP function in time. But more interestingly, the STA shows a sort of `dip', where it dives below baseline just after the PSP bump, and flares up again at the ends (a downwards slant in the initial delay period, and an upwards slant at the end of the STA).

\begin{figure}
    \subfloat{
        \includegraphics[w=0.66]{modelfit-overlay.png}
        \includegraphics[w=0.66]{modelfit-components.png}
    }
    \captionn[. ]
    {Fitting a parametric model to the STA}
    {\Left: the model $f$ (orange) fit to the STA of an example inhibitory connection from a network simulation (blue). Both signals are z-scored.
    The components of the orange model function are shown on the
    \Right: the delayed simple PSP model (blue) and  the Gaussian `dip' (orange).}
    \label{fig:modelfit}
\end{figure}

These phenomena thus differentiate an STA from a PSP. We find empirically that they can be parsimonously modelled by subtracting a broad Gaussian curve from the PSP: see \cref{fig:modelfit}, right.\footnote{
    Note that in \cref{fig:modelfit}, we fit an \emph{inhibitory} STA, and the model componentes are thus inverted: the delayed PSP bump (blue) is downwards, and the Gaussian `dip' (orange) is upwards.
}

Our model $f$ is thus the difference of a delayed version of \cref{eq:PSP}, and a  Gaussian curve. The result is scaled by some factor α, to match the size of the specific STA that is being fit. α can be negative, to model inhibitory connections; $f$ is then flipped upside-down, as in \cref{fig:modelfit}.

The model function thus has seven parameters: the PSP's delay, $τₘ$, and $τₛ$; the Gaussian's location, width, and relative height wrt.~the PSP; and the final scaling α.
All parameters, except for the Gaussian's, have a ready biophysical interpration.\footnote
    {Namely: α is proportional to the connection strength, and indicates excitation (+) or inhibition (–). The PSP's delay, $τₘ$, and $τₛ$ are estimates of the axonal transmission delay and the synaptic and membrane time constants.}

We fit the model function to individual STAs using nonlinear least-squares optimization; more specifically using the Levenberg-Marquardt algorithm.\footnote{
    This is the standard method for nonlinear least-squares optimization. For example, in \textsc{Matlab}'s \texttt{lsqcurvefit} function,  it is the default algorithm when the number of functions (in our case, 1) exceeds the number of datapoints (in our case, 1000:  100 ms of signal post-spike, at Δt = 0.1 ms).\newline
    We used the Levenberg-Marquardt implementation of the Julia package \href{https://github.com/JuliaNLSolvers/LsqFit.jl}{LsqFit.jl}.
}
%
% To make this fitting procedure work well, three things had to be done.
To make the fitted functions well-behaved and looking like actual STAs, we had to limit the creative freedom of the optimization algorithm, by enforcing box constraints on the parameters.
% - decouple params
%  (https://tfiers.github.io/phd/nb/2022-09-13__Fit_simpler_function.html)
% - 100× speedup

The final test statistic is the mean squared error (MSE) between z-scored versions of the fitted function and the STA. The normalization by z-scoring is necessary to be able to compare goodness of fit between STAs with different ranges\footnote{
    Most importantly between a real STA and its shuffled versions.
    Because those shuffled STAs have a smaller range, they will have a lower MSE, even though the fit is visually clearly worse.
}

In comparison with the previous two methods (STA height and STA template correlation), this model-fitting approach did not perform significantly better than STA height, and it was clearly worse than template correlation.

The model-fitting is also very computationally expensive: iteratively fitting a nonlinear function to an STA takes much longer than simply calculating a Pearson correlation between two STA signals.

For these two reasons, we have not performed an extensive performance evaluation of this method, as we have done for the previous and following methods (\cref{fig:N_sweep__AUC__template-corr_vs_STA,fig:N_sweep__AUC__upstroke_vs_STA} respectively).

One advantage of this model-fitting method over the other methods though, is that most of the fitted parameters have a ready biophysical explanation: connection strength, transmission delay, and synaptic and membrane time constants.




\section{Linear regression of the upstroke}

All previous methods are STA-based, i.e. different windows are cut from the postsynaptic voltage signal -- one window for every presynaptic spike -- and these are averaged into one signal, which is then used for further analysis. The method described in this section does not use STAs. We still construct the spike-triggered windows, but we do not average them. Instead, we use them to construct a gigantic data matrix for use in a linear regression.

Every single timepoint on the x-axis (in relative time after the presynaptic spike) will correspond to multiple voltage values on the y-axis; namely one for every window. Whereas for an STA, every timepoint on the x-axis corresponds to just a single y-value (the average voltage).

Why a linear regression? As we saw with the STAs in previous sections, they do not look like a line. But their first part, the "upstroke", might be approximated by one.

Let's number our spike-triggered windows $1, 2, .., N$ (for a presynaptic spiketrain with $N$ spikes), and let's number the voltage values in window $i$ as $[y_{i,1}, y_{i,2}, .., y_{i,M}]$ (for a window length of $M$ samples long).\footnote{
    This window length $M$ is an important parameter. The window must be approximately as long as the upstroke of the STA/PSP: not longer (so that the shape is too complex to be fit by a line), nor shorter (so that there would not be enough data for a proper fit).
}
We will then perform the following linear regression:
\begin{align}
    \bm{y} \hspace{1.4em}
    &=  \hspace{1.6em}
    X  \hspace{2.2em}
    \bm{β}  \hspace{0.7em}
    + \hspace{1.4em}
    \bm{ε}
    \\[1em]
    \begin{bmatrix}
        y_{1,1} \\
        y_{1,2} \\
        \vdots \\
        y_{1,M} \\
        y_{2,1} \\
        y_{2,2} \\
        \vdots \\
        \vdots \\
        y_{N,M}
    \end{bmatrix}
    &=
    \begin{bmatrix}
        1 & 1 \\
        1 & 2 \\
        \vdots & \vdots \\
        1 & M \\
        1 & 1 \\
        1 & 2 \\
        \vdots & \vdots \\
        \vdots & \vdots \\
        1 & M
    \end{bmatrix}
    \begin{bmatrix}
        β_0 \\
        β_1
    \end{bmatrix}
    +
    \begin{bmatrix}
        ε_{1,1} \\
        ε_{1,2} \\
        \vdots \\
        ε_{1,M} \\
        ε_{2,1} \\
        ε_{2,2} \\
        \vdots \\
        \vdots \\
        ε_{N,M}
    \end{bmatrix}
\end{align}
I.e. we will regress voltage-after-spike against time-after-spike.

If we assume additive Gaussian noise, i.e.
\begin{equation} \label{eq:sampling_noise}
    ε_{i,j} \sim \mathcal{N}(0, σ^2)
\end{equation}
then the maximum-likelihood estimate $\hat{\bm{β}}$ of the regression parameters $\bm{β}$ is obtained through minimizing the mean squared error (MSE)\footnote{
    \url{https://statproofbook.github.io/P/slr-mle}
}
between our observed voltage values $\bm{y}$ and the fitted line $\hat{\bm{y}} = X \hat{\bm{β}}$:
\begin{equation}
    \hat{\bm{β}} = \argmin_{\bm{β}} || \bm{y} - X \bm{β} ||_2^2.
\end{equation}
Our linear regression problem has become standard ordinary least squares (OLS) regression, for which a closed-form solution exists (the so-called normal equations: $\hat{\bm{β}} = (X^T X)^{-1} X^T \bm{y}$).
As is standard practice and for numerical stability ("don't explicitly invert a matrix if not needed"), we instead find the optimal intercept ($\hat{β}_0$) and slope ($\hat{β}_1$) of our linear fit through an implicit solution using QR-factorization, via Julia's left-division operator: \texttt{$\hat{\bm{β}}$ = X \textbackslash \ y}.

From this linear fit we thus obtain a slope $\hat{β}_1$. To use our regression as a connection test, we will perform a hypothesis test on this slope. The null hypothesis is that this slope is zero ($H_0: \hat{β} = 0$): the voltage of the tested neuron does not react to spikes of the other tested neuron, and on average it stays flat.

If the slope $β_1$ really were $0$, then we would expect the fitted slope $\hat{β}_1$ to be distributed normally around $0$, as follows:\footnote{
    \url{https://gregorygundersen.com/blog/2021/09/09/ols-hypothesis-testing/}
}
\begin{equation} \label{eq:slope_distrib}
    \hat{β}_1 \sim \mathcal{N}(0, σ^2 [Q]_{2,2})
\end{equation}
where $σ^2$ is the variance of the sampling noise (\cref{eq:sampling_noise}), and $[Q]_{2,2}$ is the second diagonal element of the 'cofactor matrix' $Q$,\footnote{
    The indices are off by one ($\hat{β}_1$ vs $[Q]_{2,2}$), because the intercept in linear regression is conventionally called $β_0$, and so we start numbering the elements of $\bm{β}$ from $0$; but we number the elements of other vectors/matrices from $1$.
}
which is the inverse of the Gram matrix $X^T X$:
\begin{equation}
    Q = (X^T X)^{-1}
\end{equation}

We do not know what the sampling noise $σ^2$ is, but we have a maximum-likelihood estimate $\hat{σ}^2$ for it,\footnote{\url{https://statproofbook.github.io/P/slr-mle}} namely the MSE of the fit:
\begin{equation}\label{eq:σhat}
    \hat{σ}^2 = || \bm{y} - X \bm{\hat{β}} ||_2^2
\end{equation}

Our final test statistic $t$ to determine whether there is a connection from neuron A
to neuron B is then the slope of the fit (of B's voltage after neuron A's spikes), normalized by how noisy the parameter fit is:
\begin{equation}
    t = \hat{β}_1 / \hat{σ}_{\hat{β}_1}
\end{equation}
where $\hat{σ}_{\hat{β}_1}$ is the standard error of the slope (\cref{eq:slope_distrib} with \cref{eq:σhat}):
\begin{equation}
    \hat{σ}_{\hat{β}_1} = \sqrt{\hat{σ}^2 [Q]_{2,2}}
\end{equation}

Unlike in \cref{eq:slope_distrib}, $t$ is no longer strictly normally distributed.\footnote{
    Instead, it follows a Student's $t$-distribution, with $n - p$ degrees of freedom, where $n = M · N$ is the number of datapoints, and $p = 2$ is the number of parameters of the regression ($β_0$ and $β_1$).
}
But when the number of datapoints is quite large, as is the case here, the $t$-distribution becomes practically normal, and our test statistic $t$ will follow the standard normal distribution under the null hypothesis ("the slope is zero").

We can (and do) use this $t$-value as-is in our connection test, for the threshold sweep.\\
Additionally, if our assumptions (linear data-generating process, additive Gaussian noise) would be correct,\footnote{They are not.}
we could also assign an actual probability (a $p$-value) to observing slopes as extreme as observed. Because $t$ follows the standard normal distribution under the null-hypothesis, and with $\phi(x)$ the cumulative probability function of this distribution, we have: $p = 2\ \phi(-|t|)$: the probability that a sample smaller than $-|t|$ or larger than $|t|$ is drawn.

% \footnote{
%     Besides the maximum-likelihood estimator $\hat{σ}^2$, there is also a Bessel-corrected estimator of $σ^2$, namely $s^2 = \frac{n}{n-p} \hat{σ}^2$ (with $p = 2$ the number of parameters in the regression). Unlike $\hat{σ}^2$, this is an unbiased estimator ($\mathbb{E}[s^2] = σ^2$); "but it has a higher MSE".
% }

% \begin{itemize}
%     \item Non-STA method: concatenated individual windows as (X, y)
%     \item Examples of pooled windows, and fits
%     \item Mention the problem of unknown transmission delays
%     \item Perfomance for different N, \& comparison with previous methods
% \end{itemize}

\Cref{fig:N_sweep__AUC__upstroke_vs_STA} shows the performance of the described linear regression method, for different number of inputs $N$, and compared to the STA height method.

\begin{figure}
    \subfloat{
        \includegraphics[w=0.72]{N_sweep__AUC__STA_height}
        \includegraphics[w=0.72]{N_sweep__AUC__upstroke}
    }
    \captionn
        {Line fit outperforms STA height as connection test}
        {AdEx neuron, 10-minute recording, no voltage imaging noise,  5 different seeds (blue dots). All inputs are tested (instead of just the highest firing ones), in addition to 100 random unconnected spiketrains. AUC chance level is at $0.252$ (as in \cref{fig:AUC_chance_level}). \\
        Source: \nburl{2023-04-11__Nto1_AdEx_conntest_methods_comparison}.}
    \label{fig:N_sweep__AUC__upstroke_vs_STA}
\end{figure}



% \section{Clustering, \& Hierarchical model fitting}

% \begin{itemize}
%     \item (Time-permitting)
% \end{itemize}


% \section{Zhou/Cai's `Spike-triggered regression'}

% \begin{itemize}
%     \item (Time-permitting)
% \end{itemize}


\FloatBarrier
\section{Computational cost}

\begin{itemize}
    \item Timings of each method, extrapolation for larger number of tested connections
\end{itemize}

\section{Summary}

\begin{itemize}
    \item Conclusions of the N-to-1 experiment
    \item Leadup to the network experiments: what we could not yet test (the problem of indirect connections, as e.g. identified in the connectomics challenge)
\end{itemize}
