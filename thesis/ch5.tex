% \begin{itemize}
%     \item Network connectivity, E/I balance, raster plots
%     \item Too many possible connections to test them all → Subsampling
%     \item Performance of last chapter's methods
%     \item If time: experiment with a network that is less densely connected than our current fully-random one. Why? To better examine the effect of indirect connections / colliders (For the current connectivity, there are too many of those. But in a more realistic, 'localized' network, there are less, and so it seems easier to isolate and examine their effect).
% \end{itemize}

\section{Introduction}

In the previous experiments, only one neuron's voltage was simulated. The inputs were Poisson spike trains.
In the next experiments, we simulate the voltages of a full network of neurons, which are recurrently connected to each other.
The goal is to investigate the effect on network inference of potentially correlated inputs and indirect connections.




\section{Connectivity structure}

\begin{figure}
    \subfloat{
        \includegraphics[w=0.75]{gephi-direct-inputs.png}
        \includegraphics[w=0.75]{gephi-inputs-to-inputs.png}
    }
    \vspace*{2em}
    \caption
        {\textbf{A neuron is reachable in two hops from most other neurons}.    Selected neurons and connections in our random network. Left shows the direct inputs to one of the neurons. Right additionally shows the direct inputs to these inputs. This subnetwork already contains more than 700 of the 1000 total neurons in the network.\\
        Excitatory neurons in green, inhibitory in red. The tangle in the middle consists of neurons that synapse onto multiple of the direct inputs of our selected neuron.\\
        Visualization using the `Gephi' software, with the `Yifan-Hu' layout algorithm, and default parameters otherwise.
        Source: \nburl{2022-08-29__Visualizing_subnets}.}
    \label{fig:gephi-network-viz}
\end{figure}

% These two margin figs can go to Appendix
\marginpar{
    \includegraphics[w=1, trim={0 0.4em 0 -1em}, clip]{shortest-path-1}
    \captionof{figure}{
        \textbf{A selected neuron is reachable in at most three hops.}
        Shortest path lengths from every other neuron in the network.}
    \label{fig:shortest-path-1}
}
\marginpar{
    \includegraphics[w=1, trim={0 0.4em 0 -1em}, clip]{shortest-path-all}
    \captionof{figure}{
        \textbf{Every neuron is reachable in at most three hops.}
        Shortest path lengths calculated using the Floyd-Warshall algorithm. Source: \nburl{2022-07-14__Unconnected-but-detected.html}.}
    \label{fig:shortest-path-all}
}

We choose the simple and common `fully random' connectivity rule,\footnote{
    Other common choices for connectivity structure are scale-free networks, and `local' networks.
}
where any neuron has a connection to another with a uniform random probability (we choose $p_\text{conn} = 0.04$). After generating an adjacency matrix this way (\verb|A = rand(N, N) .≤ 0.04|, where \verb|rand| draws from $\sim U[0,1]$), we remove autapses. We choose the number of neurons $N = 1000$.

A property of fully random networks is that they are strongly interconnected. In our network, any neuron is reachable from any other in at most three steps (three synapses); most are reachable in just two. This is exemplified in \cref{fig:gephi-network-viz} and \cref{fig:shortest-path-1}: one selected neuron (neuron `1' here) is reachable in two hops from more than 700 of the 1000 total neurons in the network. And when we compute the shortest path between every possible neuron pair, we find a very similar distribution (\cref{fig:shortest-path-all}).




\section{External input}

As we no longer have Poisson spike trains providing input to our neurons, we need another way of bootstrapping activity in the network.\\
Instead of external spikes, each neuron is provided with external input by adding Gaussian noise to its membrane voltage. Every time step ($Δt = 0.1$~ms), a sample drawn from a normal distribution with mean $–0.5$~pA and $σ = 5$~pA is added to the membrane current. (As membrane current is by convention negative, this corresponds to an on-average positive influence on membrane voltage).



\section{EI balance}

Similar to the N-to-1 experiments, we make 1 out of 5 neurons inhibitory.
As before, this is done by setting the synaptic reversal potential at the outputs of inhibitory neurons to $-80$ mV (instead of the $0$ mV for excitatory neurons).
To make sure that each neuron receives a balanced mix of excitation and inhibition, and given that there are 4:1 excitatory to inhibitory neurons, we make excitatory neurons 4x weaker: their synaptic strength ($Δg$, the instantaneous increase in postsynaptic conductance $g$ on spike arrival) is 4x as small as that of inhibitory neurons.

% Option for appendix, these four figs
\begin{figure}
    \subfloat{
        \includegraphics[w=0.74]{1144_raster}
        \includegraphics[w=0.74]{1144_hist}
    }
    \captionn
        {Firing rates in the network with `1144' weights}
        {Left: rasterplot showing all spikes in the first 10 seconds of the simulation.\\
        Right: histogram of firing rates.\\
        Source: \nburl{2022-07-01__g_EI}.}
    \label{fig:1144-weights}
\end{figure}

\begin{figure}
    \subfloat{
        \includegraphics[w=0.74]{roxin_raster}
        \includegraphics[w=0.74]{roxin_hist}
    }
    \captionn
        {Firing rates in the network with `Roxin2011' weights}
        {See \cref{fig:1144-weights}.}
    \label{fig:roxin-weights}
\end{figure}

% A synaptic weight rule where inhibitory synapses are 4× stronger to compensate for there being 4× as many synapses.
% Given that we simulated four times as many excitatory as inhibitory neurons, the synaptic weight rule described above (with inhibitory synapses 4× as strong) seems logical, to keep excitation and inhibition in balance.
\Cref{fig:1144-weights} shows results of the network simulation as spiketrains and firing rate distributions, using the described synaptic weights rule (inhibitory synapses 4× stronger than excitatory ones). We find that the resulting firing rate distribution is fairly symmetrical, and that excitatory and inhibitory neurons have very similar firing rates.

In real neural networks however, inhibitory neurons often have higher firing rates than their excitatory neighbours. In addition, real firing rate distributions are often heavy-tailed and not symmetrical, as we saw back in \cref{sec:lognormal-poisson-input}.
So, to coax more realistic firing rate distributions out of our network, we looked at the previously mentioned modelling paper of Roxin \emph{et al}, ``On the Distribution of Firing Rates in Networks of Cortical Neurons'', \cite{Roxin2011DistributionFiringRates}.\footnote{
    This paper seeks to explain how heavy-tailed firing rate distributions emerge in randomly connected spiking neural networks. We wanted to emulate the simulations in this work, to obtain a heavy-tailed firing rate distribution.
}
% Their figures indeed seem to show a lognormal-like distribution of firing rates from their simulatd networks.
In this paper different synaptic weights are used than the ones described above.
Normalizing excitatory-to-excitatory (\verb|E→E|) connections to 1, Roxin et al's synaptic weights are:
\begin{verbatim}
    E→E: 1
    E→I: 18 (instead of 1)
    I→E: 36 (instead of 4)
    I→I: 31 (instead of 4)
\end{verbatim}
We will call these weights `Roxin2011'. The naive, `balancing' weights used in \cref{fig:1144-weights}, we will call `1144'.

Simulating a network with the `Roxin2011' weights indeed increases the firing rates of inhibitory neurons with respect to those of excitatory neurons (\cref{fig:roxin-weights}). The two firing rate distributions are however both as symmetrical as before.\footnote{
    When collating the two firing rate distributions from both neuron types, the resulting single distribution \emph{is} heavy-tailed, and even vaguely looks log-normal (but it is not). In \cite{Roxin2011DistributionFiringRates}, similar-looking, collated firing rate distributions are shown, and are called lognormal (but this is never quantified in the paper).
}

This imbalance between \verb|E→E| and \verb|E→I| synapses is not that rare: another modelling paper that also investigated EI-balance, \cite{Sadeh2021ExcitatoryinhibitoryBalanceModulates}, has the following synaptic weights. If our original weights are $[1, 1, 4, 4]$, then Sadeh and Clopath's weights are $[1, k, k, k]$, with $k = 4$ (in their "strong E-I coupling regime").

In the results that follow, we use both `1144' and the `Roxin2011' synaptic weights.

Finally, we also looked at drawing synaptic weights from a distribution (specifically, a lognormal one) instead of setting all weights of one type to be equal. This did not change the symmetry of the resulting firing rate distributions, so we did not pursue this further.


\section{Subsampling}

We simulate all 1000 neurons' voltages, but, to save memory and disk space,  do not record all these traces.\footnote{
    For a 10-minute simulation with a timestep of 0.1 ms, one voltage trace takes 48 MB (at 64 bit per sample). Our 1000 neurons thus take 48 GB -- and that is just for one simulation (one set of parameters).\newline
    We \emph{do} record the spike trains of all neurons. Saving just spike times takes considerably less space: a neuron spiking at 10 Hz for 10 minutes emits 6000 spikes, which, at 64 bit per timestamp, takes just  48 kB.\newline
    In other words,  at a 0.1 ms sample rate, a spike train occupies about 1000× less memory than the corresponding voltage trace.
}
In most experiments here, we recorded the voltage traces of 40 excitatory and 10 inhibitory neurons (5\% of all neurons).

Additionally, when performing connection tests on the inputs of a recorded neuron, we do not test the spiketrains of all 999 other neurons. Instead, we test only a (biased) sample of the possible inputs, to save processing time.
This sample is constructed as follows. We test all the a-priori known true direct inputs -- both excitatory and inhibitory -- and add a random sample of 40 not-directly-connected neurons.

On average, each neuron has ± 32 excitatory inputs and ± 8 inhibitory inputs. This means that, from the 1000 × 1000 possible connections, we only test about 4000, or 0.4\%.\footnote{
    Calculation behind these numbers:\\
    - 1000 neurons × 80\% excitatory × 4\% probability of an input connection = 32 excitatory inputs on average.\\
    - 50 `post' neurons (40 excitatory and 10 inhibitory voltage-recorded neurons) × 80 `pre' neurons (±40 connected + 40 unconnected) = 4000 tested connections.\\
    - To be precise, instead of 1000 × 1000, there are rather  $1000^2 - 1000$ possible connections, as we would not test for autapses.
}

\begin{figure}
    \subfloat{
        \includegraphics[w=0.74]{2022-09-01__1144_weights_23_0.png}
        \includegraphics[w=0.74]{2022-09-01__1144_weights_24_0.png}
    }
    \captionn[. ]
    {Performance of the STA `peak-to-peak' connection test in the random network}
    {10 minute recording, with `1144' weights. Each dot is the true positive rate for one`post' neuron. The gray dotted line indicates the p-value threshold α of 0.05.\\
    Source: \nburl{2022-09-01__1144_weights}.}
    \label{fig:net-perf}
    \vspace*{2em}
\end{figure}


\section{Connection testing}

We tested connections using the STA height (or `peak-to-peak') test. In the chronology of the PhD, the more advanced methods presented in the previous chapter were developed only after the network tests in this chapter were done. Applying the newer methods to the full network (and not just the N-to-1 setup) is a topic for further work.
% also: detrates at fixed threshold, instead of AUC.

\Cref{fig:net-perf} shows the performance of using the STA height as connection test in the network. The detection rates are shown at a fixed detection threshold $α$ of $0.05$. That is, a connection was classified as real if its STA had a larger height than 95 of the 100 `control' STAs, which where made by randomly shuffling the presynaptic spiketrains.

Detection rates are broken down per type of both the presynaptic and postsynaptic neuron (excitatory or inhibitory). We find that inhibitory inputs are significantly easier to detect (independent of the type of the postsynaptic neuron). This is due to the synaptic weight of inhibitory inputs -- and thus their PSPs -- being $4×$ stronger than those of excitatory inputs.

% next is: more inhibition.
